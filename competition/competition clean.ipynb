{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Here is the jupyter notebook for those who have signed up for the [competition](https://codalab.lisn.upsaclay.fr/competitions/8440?secret_key=51d5952f-d68d-47d9-baef-6032445dea01) in NTU.\n",
    "\n",
    "## Goal\n",
    "The competition's goal is to maximize the accumulative return rate of the test input. Based on the current price information, your agent should generate a portfolio weight which will give the hightest return rate defined below. \n",
    "\n",
    "## Dataset\n",
    "In this competition, you will be given 1 complete datasets as training dataset and 2 incomplete datasets as test datasets, where there is no feature `close`.\n",
    "\n",
    "In both complete and incomplete datasets, there are 15 assets.\n",
    "### Data Visualization\n",
    "Here is a glance of how the train data look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "      <th>tic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.008320</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>-0.010062</td>\n",
       "      <td>-0.143726</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>-0.010348</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>-0.061124</td>\n",
       "      <td>-0.072128</td>\n",
       "      <td>-0.126481</td>\n",
       "      <td>0</td>\n",
       "      <td>14.765714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>-0.005646</td>\n",
       "      <td>-0.225326</td>\n",
       "      <td>-0.005459</td>\n",
       "      <td>0.007794</td>\n",
       "      <td>-0.008521</td>\n",
       "      <td>-0.008222</td>\n",
       "      <td>-0.017393</td>\n",
       "      <td>-0.031253</td>\n",
       "      <td>-0.042012</td>\n",
       "      <td>0</td>\n",
       "      <td>63.759998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004561</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>-0.017413</td>\n",
       "      <td>-0.136140</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>-0.004885</td>\n",
       "      <td>-0.017185</td>\n",
       "      <td>-0.029824</td>\n",
       "      <td>-0.045096</td>\n",
       "      <td>-0.052918</td>\n",
       "      <td>0</td>\n",
       "      <td>48.240002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003094</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>-0.009956</td>\n",
       "      <td>-0.181742</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.002922</td>\n",
       "      <td>-0.007421</td>\n",
       "      <td>-0.004776</td>\n",
       "      <td>-0.002812</td>\n",
       "      <td>-0.013054</td>\n",
       "      <td>-0.001687</td>\n",
       "      <td>0</td>\n",
       "      <td>74.330002</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.015182</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>-0.015182</td>\n",
       "      <td>-0.252029</td>\n",
       "      <td>0.009257</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>-0.004803</td>\n",
       "      <td>-0.010627</td>\n",
       "      <td>-0.001714</td>\n",
       "      <td>-0.005438</td>\n",
       "      <td>0.006133</td>\n",
       "      <td>0</td>\n",
       "      <td>94.849998</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  -0.008320   0.002999  -0.010062  -0.143726   0.005374  -0.010348   \n",
       "0   0.005332   0.005803  -0.005646  -0.225326  -0.005459   0.007794   \n",
       "0  -0.004561   0.002902  -0.017413  -0.136140  -0.003100   0.004561   \n",
       "0  -0.003094   0.003632  -0.009956  -0.181742   0.001482   0.002922   \n",
       "0  -0.015182   0.006958  -0.015182  -0.252029   0.009257   0.010322   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  feature_10  date      close  \\\n",
       "0  -0.011415   0.002751  -0.061124  -0.072128   -0.126481     0  14.765714   \n",
       "0  -0.008521  -0.008222  -0.017393  -0.031253   -0.042012     0  63.759998   \n",
       "0  -0.004885  -0.017185  -0.029824  -0.045096   -0.052918     0  48.240002   \n",
       "0  -0.007421  -0.004776  -0.002812  -0.013054   -0.001687     0  74.330002   \n",
       "0  -0.004803  -0.010627  -0.001714  -0.005438    0.006133     0  94.849998   \n",
       "\n",
       "   tic  \n",
       "0    0  \n",
       "0    1  \n",
       "0    2  \n",
       "0    3  \n",
       "0    4  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "train=pd.read_csv(\"data/train.csv\",index_col=0)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the RL setting, we view all 15 stocks's feature information as state, which is the input of your model.\n",
    "\n",
    "Here is a glance of what the state in timestamp 0 looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical_indicator_list=[\"feature_0\",\"feature_1\",\"feature_2\",\"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\",\"feature_7\",\"feature_8\",\"feature_9\",\"feature_10\"]\n",
    "# train.loc[0][technical_indicator_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Calculation\n",
    "Based on the state in timestamp 0, we use random portfolio weights to demonstrate how the reward of the random action is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate random action\n",
    "# random_score=np.random.rand(15)\n",
    "# random_weights=np.exp(random_score)/np.sum(np.exp(random_score))\n",
    "# # calculate return_rate for every single ticker of day 0\n",
    "# close_price_0=train.loc[0].close.values\n",
    "# close_price_1=train.loc[1].close.values\n",
    "# single_return_rate=close_price_1/close_price_0-1\n",
    "# # calculate the return rate of the random action\n",
    "# return_rate_action=np.sum(random_weights*single_return_rate)\n",
    "# return_rate_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a [Trading Environment](https://github.com/TradeMaster-NTU/TradeMaster/blob/main/env/PM/portfolio_for_EIIE.py) to automate the process of state stepping and reward calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Implementation\n",
    "Here, we provide a baseline for the competition using one of the algorithms in TradeMaster. You can follow the rest jupyter notebook to create a borderline submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Here we provide the [video tutorial](https://www.youtube.com/watch?v=7rtqFT9I4uo&t=12s) for you to install this project to better participate in the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL enviornment & baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Environment Set Up\n",
    "First, we need to add the project to our system path because the setting of jupter notebook is a little different from that of py document.\n",
    "Then, we need to load the document we need to import the module we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from agent.EIIE.model import EIIE_con, EIIE_lstm, EIIE_rnn, EIIE_critirc\n",
    "import argparse\n",
    "from agent.EIIE.util import *\n",
    "from env.PM.portfolio_for_EIIE_clean import Tradingenv\n",
    "from logging import raiseExceptions\n",
    "from stat import S_ENFMT\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import sys\n",
    "from agent.EIIE.trader import trader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config and Hyperparameters Adjustment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a part where you can adjust the default hyperparameters\n",
    "More specifically, besides the hyperparameters the DPG need, it also contains information like the path of the config file of the training and validing environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_epoch'], dest='num_epoch', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='the number of epoch we train', metavar=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--random_seed\",\n",
    "                    type=int,\n",
    "                    default=12345,\n",
    "                    help=\"random seed number\")\n",
    "parser.add_argument(\n",
    "    \"--env_config_path\",\n",
    "    type=str,\n",
    "    default=\"config/input_config/env/portfolio/portfolio_for_EIIE/\",\n",
    "    help=\"the path for storing the downloaded data\")\n",
    "parser.add_argument(\n",
    "    \"--net_type\",\n",
    "    choices=[\"conv\", \"lstm\", \"rnn\"],\n",
    "    default=\"rnn\",\n",
    "    help=\"the name of the model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_hidden_nodes\",\n",
    "    type=int,\n",
    "    default=32,\n",
    "    help=\"the number of hidden nodes in lstm or rnn\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_out_channel\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"the number of channel\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gamma\",\n",
    "    type=float,\n",
    "    default=0.99,\n",
    "    help=\"the gamma for DPG\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_path\",\n",
    "    type=str,\n",
    "    default=\"result/EIIE/trained_model\",\n",
    "    help=\"the path for trained model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--result_path\",\n",
    "    type=str,\n",
    "    default=\"result/EIIE/test_result\",\n",
    "    help=\"the path for test result\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epoch\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"the number of epoch we train\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Building & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df_path': 'data/train.csv', 'initial_amount': 100000, 'length_day': 3, 'tech_indicator_list': ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10'], 'transaction_cost_pct': 0.0}\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=[])\n",
    "agent=trader(args)\n",
    "# agent.train_with_valid()\n",
    "\n",
    "print(agent.train_env_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing & Generate Submission Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_information_1=pd.read_csv(\"data/test_input_1.csv\",index_col=0)\n",
    "# test_information_2=pd.read_csv(\"data/test_input_2.csv\",index_col=0)\n",
    "# technical_indicator=[\"feature_0\",\"feature_1\",\"feature_2\",\"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\",\"feature_7\",\"feature_8\",\"feature_9\",\"feature_10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_list_1=[]\n",
    "# for date in test_information_1.index.unique():\n",
    "#     s=test_information_1[test_information_1.index==date][technical_indicator].values\n",
    "#     shape=s.shape\n",
    "#     s=s.reshape(shape[0],1,shape[1])\n",
    "#     s=torch.from_numpy(s).float()\n",
    "#     action=agent.net(s)\n",
    "#     #here the origional action for the environment consider the cash, which is more pracitical in real world, but for \n",
    "#     #the competition, we only need the last 15 weights for the assets, therefore we need to normalize the result as well\n",
    "#     action=action.detach().float().numpy()\n",
    "#     action=action[1:]/np.sum(action[1:])\n",
    "#     action_list_1.append(action)\n",
    "# action_list_1=np.array(action_list_1)\n",
    "# action_list_1=action_list_1.astype(float)\n",
    "# np.save(\"result/EIIE/action/action1.npy\",action_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_list_2=[]\n",
    "# for date in test_information_2.index.unique():\n",
    "#     s=test_information_2[test_information_2.index==date][technical_indicator].values\n",
    "#     shape=s.shape\n",
    "#     s=s.reshape(shape[0],1,shape[1])\n",
    "#     s=torch.from_numpy(s).float()\n",
    "#     action=agent.net(s)\n",
    "#     #here the origional action for the environment consider the cash, which is more pracitical in real world, but for \n",
    "#     #the competition, we only need the last 15 weights for the assets, therefore we need to normalize the result as well\n",
    "#     action=action.detach().float().numpy()\n",
    "#     action=action[1:]/np.sum(action[1:])\n",
    "#     action_list_2.append(action)\n",
    "# action_list_2=np.array(action_list_2)\n",
    "# action_list_2=action_list_2.astype(float)\n",
    "# np.save(\"result/EIIE/action/action2.npy\",action_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Compress the 2 file as a zip file as submission file.\n",
    "Please notice that the zip file should only contain this 2 files name after `action1.npy` and `action2.npy` with no external folder. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next is core code of the agent.\n",
    "## RL Environment Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TradeMaster, you need to provide 3 RL environment configuration before building an agent. The TradeMaster agents will automatically follow the process using the training RL environment to train the agent, test it in the validation environment and pick the best model of training and pose it in the testing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_instance=agent.train_env_instance\n",
    "valid_env_instance=agent.valid_env_instance\n",
    "test_env_instance=agent.train_env_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the environment, means to clear all the history and start from the begining. \n",
    "# It will return the initial state  \n",
    "# Here is an example of posing the random action to the train_env_instance\n",
    "# s=train_env_instance.reset()\n",
    "# action=np.random.rand(16)\n",
    "# done=False\n",
    "# while not done:\n",
    "#     old_state = s\n",
    "#     s, reward, done, _ =train_env_instance.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_action_score(action_score_list):\n",
    "    true_action_list = []\n",
    "    for action_score in action_score_list:\n",
    "        if np.sum(action_score) == 1:\n",
    "            action = action_score\n",
    "        else:\n",
    "            action = np.exp(action_score) / np.sum(np.exp(action_score))\n",
    "        true_action_list.append(action)\n",
    "    return np.array(true_action_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is the core code of EIIE, I will decompose it so that you can understand the process of training, making it easier for you to build your own agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the net\n",
    "# Just like the supervised leanrning process, we need a net to regress something, here the EIIE is Actor-Critic RL model where we need a actor to to generate policy and a critic\\\n",
    "# to judge whether the state is good or not\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EIIE_con(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, length, kernel_size=1):\n",
    "        super(EIIE_con, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.length = length\n",
    "        self.act = torch.nn.ReLU(inplace=False)\n",
    "        self.con1d = nn.Conv1d(self.in_channels,\n",
    "                               self.out_channels,\n",
    "                               kernel_size=1)\n",
    "        self.con2d = nn.Conv1d(self.out_channels,\n",
    "                               1,\n",
    "                               kernel_size=1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.con1d(x)\n",
    "        x = self.act(x)\n",
    "        x = self.con2d(x)\n",
    "        x = self.act(x)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "\n",
    "        # self.linear2 = nn.Linear(len(x), len(x) + 1)\n",
    "        # x = self.linear2(x)\n",
    "        x = torch.cat((x, self.para), dim=0)\n",
    "        x = torch.softmax(x, dim=0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EIIE_lstm(nn.Module):\n",
    "    def __init__(self, n_features, layer_num, n_hidden):\n",
    "        super(EIIE_lstm, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = layer_num\n",
    "        self.lstm = nn.LSTM(input_size=n_features,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # print('1',lstm_out.shape)\n",
    "        # print('2',lstm_out[:, :, :].shape)\n",
    "        # x = self.linear(lstm_out).transpose(1, 2)\n",
    "        x = self.linear(lstm_out[:, -1, :]).view(-1, 1, 1)\n",
    "        # print('3',x.transpose(1, 2).shape)        \n",
    "        # print('4',x.shape)\n",
    "        # print('5',x)\n",
    "        x = self.con3d(x)\n",
    "        # print('6',x.shape)\n",
    "        x = x.view(-1)\n",
    "        # print('7: ',x.shape)\n",
    "        # x = torch.cat((x, self.para), dim=0)\n",
    "        # print('8: ',x.shape)        \n",
    "        x = torch.softmax(x, dim=0)\n",
    "        # print(x.shape)\n",
    "        # print('in: ', x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EIIE_rnn(nn.Module):\n",
    "    def __init__(self, n_features, layer_num, n_hidden):\n",
    "        super(EIIE_rnn, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = layer_num\n",
    "        self.rnn = nn.RNN(input_size=n_features,\n",
    "                          hidden_size=self.n_hidden,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.rnn(x)\n",
    "        x = self.linear(lstm_out[:, -1, :]).view(-1, 1, 1)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "        x = torch.cat((x, self.para), dim=0)\n",
    "        x = torch.softmax(x, dim=0)\n",
    "        return x\n",
    "    \n",
    "class EIIE_critirc(nn.Module):\n",
    "    def __init__(self, n_features, layer_num, n_hidden):\n",
    "        super(EIIE_critirc, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = layer_num\n",
    "        self.lstm = nn.LSTM(input_size=n_features,\n",
    "                            hidden_size=self.n_hidden,\n",
    "                            num_layers=self.n_layers,\n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(self.n_hidden, 1)\n",
    "        self.con3d = nn.Conv1d(1, 1, kernel_size=1)\n",
    "        self.para = torch.nn.Parameter(torch.ones(1).requires_grad_())\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        x = self.linear(lstm_out[:, -1, :]).view(-1, 1, 1)\n",
    "        x = self.con3d(x)\n",
    "        x = x.view(-1)\n",
    "        x = torch.cat((x, self.para, a), dim=0)\n",
    "        x = torch.nn.ReLU(inplace=False)(x)\n",
    "        number_nodes = len(x)\n",
    "        self.linear2 = nn.Linear(number_nodes, 1)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are the code for the trader, the key lies on the learn function\n",
    "model_name = \"no_params\"\n",
    "# model_name = \"lstm_win3_noStandardized_2layer_16hidden_0.99g\"\n",
    "result_folder_location = \"result/\"+ model_name + \"/test_result/\"\n",
    "model_folder_location = \"result/\"+ model_name + \"/trained_model\"\n",
    "class trader:\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 7\n",
    "        self.GPU_IN_USE = torch.cuda.is_available()\n",
    "        self.device = torch.device('cpu' if self.GPU_IN_USE else 'cpu')\n",
    "        self.model_path = model_folder_location\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        self.result_path = result_folder_location\n",
    "        if not os.path.exists(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "        self.train_env_instance = train_env_instance\n",
    "        self.valid_env_instance = valid_env_instance\n",
    "        self.test_env_instance = test_env_instance\n",
    "        self.n_hidden = 16\n",
    "        self.input_channel = 11\n",
    "        self.layer_num = 2\n",
    "        self.net = EIIE_lstm(self.input_channel, self.layer_num,\n",
    "                           self.n_hidden)\n",
    "        \n",
    "        self.critic = EIIE_critirc(self.input_channel, self.layer_num,\n",
    "                                  self.n_hidden)\n",
    "        self.test_action_memory = []  # to store the\n",
    "        self.optimizer_actor = torch.optim.Adam(self.net.parameters(), lr=1e-4)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=1e-4)\n",
    "        self.memory_counter = 0\n",
    "        self.memory_capacity = 1000\n",
    "        self.s_memory = []\n",
    "        self.a_memory = []\n",
    "        self.r_memory = []\n",
    "        self.sn_memory = []\n",
    "        self.policy_update_frequency = 500\n",
    "        self.critic_learn_time = 0\n",
    "        self.gamma = 0.99\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.critic = self.critic.to(self.device)\n",
    "\n",
    "    def store_transition(\n",
    "        self,\n",
    "        s,\n",
    "        a,\n",
    "        r,\n",
    "        s_,\n",
    "    ):  # 定义记忆存储函数 (这里输入为一个transition)\n",
    "\n",
    "        self.memory_counter = self.memory_counter + 1\n",
    "        if self.memory_counter < self.memory_capacity:\n",
    "            self.s_memory.append(s)\n",
    "            self.a_memory.append(a)\n",
    "            self.r_memory.append(r)\n",
    "            self.sn_memory.append(s_)\n",
    "        else:\n",
    "            number = self.memory_counter % self.memory_capacity\n",
    "            self.s_memory[number - 1] = s\n",
    "            self.a_memory[number - 1] = a\n",
    "            self.r_memory[number - 1] = r\n",
    "            self.sn_memory[number - 1] = s_\n",
    "\n",
    "    def compute_single_action(self, state):\n",
    "        state = torch.from_numpy(state).float().to(self.device)\n",
    "        action = self.net(state)\n",
    "        action = action.detach().cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # here is the core of the trader, it shows how the updates coming out\n",
    "        # we first need to have some stored the transcation(s,a,r,s_) \n",
    "        length = len(self.s_memory)\n",
    "        out1 = random.sample(range(length), int(length / 10))\n",
    "        # random sample\n",
    "        s_learn = []\n",
    "        a_learn = []\n",
    "        r_learn = []\n",
    "        sn_learn = []\n",
    "        for number in out1:\n",
    "            s_learn.append(self.s_memory[number])\n",
    "            a_learn.append(self.a_memory[number])\n",
    "            r_learn.append(self.r_memory[number])\n",
    "            sn_learn.append(self.sn_memory[number])\n",
    "        self.critic_learn_time = self.critic_learn_time + 1\n",
    "        # for the transcation we have stored, we need to update the actor and critic\n",
    "        # for the actor, we need to comput the action and use the critic to judge the action\n",
    "        # we need to update the actor so that for every action it choose, it can gain more scores from a critic than other action \n",
    "        # for the critic , we simply use the td_error to update it because it is MDP\n",
    "\n",
    "        for bs, ba, br, bs_ in zip(s_learn, a_learn, r_learn, sn_learn):\n",
    "            #update actor\n",
    "            a = self.net(bs)\n",
    "            q = self.critic(bs, a)\n",
    "            a_loss = -torch.mean(q)\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            a_loss.backward(retain_graph=True)\n",
    "            self.optimizer_actor.step()\n",
    "            #update critic\n",
    "            a_ = self.net(bs_)\n",
    "            q_ = self.critic(bs_, a_.detach())\n",
    "            q_target = br + self.gamma * q_\n",
    "            q_eval = self.critic(bs, ba.detach())\n",
    "            # print(q_eval)\n",
    "            # print(q_target)\n",
    "            td_error = self.mse_loss(q_target.detach(), q_eval)\n",
    "            # print(td_error)\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            td_error.backward()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "    def train_with_valid(self):\n",
    "        # print(self.train_env_instance.length_day)\n",
    "        rewards_list = []\n",
    "        for i in range(self.num_epoch):\n",
    "            j = 0\n",
    "            done = False\n",
    "            s = self.train_env_instance.reset()\n",
    "            while not done:\n",
    "\n",
    "                old_state = s\n",
    "                # print('checking input size: ', torch.from_numpy(s).float().size())\n",
    "                action = self.net(torch.from_numpy(s).float())\n",
    "                # print('out: ', action.shape)\n",
    "                s, reward, done, _ = self.train_env_instance.step(\n",
    "                    action.detach().numpy())\n",
    "                self.store_transition(\n",
    "                    torch.from_numpy(old_state).float().to(self.device),\n",
    "                    action,\n",
    "                    torch.tensor(reward).float().to(self.device),\n",
    "                    torch.from_numpy(s).float().to(self.device))\n",
    "                j = j + 1\n",
    "                if j % 100 == 1:\n",
    "                    print(f\"epoch {i} step {j}:\")\n",
    "                    print(f\"action: {action}\")\n",
    "                    print(f\"reward: {reward}\")\n",
    "                    self.learn()\n",
    "            all_model_path = self.model_path + \"/all_model/\"\n",
    "            best_model_path = self.model_path + \"/best_model/\"\n",
    "            if not os.path.exists(all_model_path):\n",
    "                os.makedirs(all_model_path)\n",
    "            if not os.path.exists(best_model_path):\n",
    "                os.makedirs(best_model_path)\n",
    "            torch.save(self.net,\n",
    "                       all_model_path + \"actor_num_epoch_{}.pth\".format(i))\n",
    "            torch.save(self.critic,\n",
    "                       all_model_path + \"critic_num_epoch_{}.pth\".format(i))\n",
    "            s = self.valid_env_instance.reset()\n",
    "            done = False\n",
    "            rewards = 0\n",
    "            while not done:\n",
    "\n",
    "                old_state = s\n",
    "                action = self.net(torch.from_numpy(s).float())\n",
    "                # print('out2: ', action.shape)\n",
    "                s, reward, done, _ = self.valid_env_instance.step(\n",
    "                    action.detach().numpy())\n",
    "                rewards = rewards + reward\n",
    "            rewards_list.append(rewards)\n",
    "        max_reward = np.max(rewards_list)\n",
    "        index = rewards_list.index(max_reward)\n",
    "        actor_model_path = all_model_path + \"actor_num_epoch_{}.pth\".format(\n",
    "            index)\n",
    "        critic_model_path = all_model_path + \"critic_num_epoch_{}.pth\".format(\n",
    "            index)\n",
    "        self.net = torch.load(actor_model_path)\n",
    "        self.critic = torch.load(critic_model_path)\n",
    "        os.makedirs(best_model_path + str(max_reward))\n",
    "        torch.save(self.net, best_model_path + \"actor.pth\")\n",
    "        torch.save(self.critic, best_model_path + \"critic.pth\")\n",
    "\n",
    "    def test(self):\n",
    "        s = self.test_env_instance.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            old_state = s\n",
    "            action = self.net(torch.from_numpy(s).float())\n",
    "            s, reward, done, _ = self.test_env_instance.step(\n",
    "                action.detach().numpy())\n",
    "        df_return = self.test_env_instance.save_portfolio_return_memory()\n",
    "        df_assets = self.test_env_instance.save_asset_memory()\n",
    "        assets = df_assets[\"total assets\"].values\n",
    "        daily_return = df_return.daily_return.values\n",
    "        df = pd.DataFrame()\n",
    "        df[\"daily_return\"] = daily_return\n",
    "        df[\"total assets\"] = assets\n",
    "        if not os.path.exists(self.result_path):\n",
    "            os.makedirs(self.result_path)\n",
    "        df.to_csv(self.result_path + \"/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 1:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.003429663850077702\n",
      "epoch 0 step 101:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0290366405276945\n",
      "epoch 0 step 201:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014295729681242264\n",
      "epoch 0 step 301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008275149029280726\n",
      "epoch 0 step 401:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006352449093295576\n",
      "epoch 0 step 501:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.003118358195340676\n",
      "epoch 0 step 601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0013243282922257293\n",
      "epoch 0 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.021136059334581958\n",
      "epoch 0 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002290713316465798\n",
      "epoch 0 step 901:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0023777727218465827\n",
      "epoch 0 step 1001:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0072749381070771335\n",
      "epoch 0 step 1101:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012949188276053292\n",
      "epoch 0 step 1201:\n",
      "action: tensor([0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006020272374046343\n",
      "epoch 0 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0026630574261314877\n",
      "epoch 0 step 1401:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0011661640215869795\n",
      "epoch 0 step 1501:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0011987628575376164\n",
      "epoch 0 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0666, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006389381789571047\n",
      "epoch 0 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.03492851021415788\n",
      "epoch 0 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.00931752296644106\n",
      "epoch 0 step 1901:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002789960125554103\n",
      "epoch 0 step 2001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0054389470062634615\n",
      "=================================\n",
      "the profit margin is 205.76029866385005 %\n",
      "the sharpe ratio is 2.96476762262976\n",
      "the Volatility is 0.009030129497752941\n",
      "the max drawdown is 0.6740966811476274\n",
      "the Calmar Ratio is 1.7801328600626471\n",
      "the Sortino Ratio is 3.887698808101529\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.7474395531424 %\n",
      "the sharpe ratio is 2.9646784765263243\n",
      "the Volatility is 0.009030077334279673\n",
      "the max drawdown is 0.6740829743219441\n",
      "the Calmar Ratio is 1.7801052473166654\n",
      "the Sortino Ratio is 3.8877844936450106\n",
      "=================================\n",
      "epoch 1 step 1:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.003429760770661261\n",
      "epoch 1 step 101:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0666, 0.0667, 0.0668,\n",
      "        0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.029035227506334138\n",
      "epoch 1 step 201:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014295784592656346\n",
      "epoch 1 step 301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008274917979617769\n",
      "epoch 1 step 401:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0668, 0.0667, 0.0666, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006353870036640785\n",
      "epoch 1 step 501:\n",
      "action: tensor([0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0031193653683967426\n",
      "epoch 1 step 601:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0013252263873422265\n",
      "epoch 1 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0211391860714496\n",
      "epoch 1 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0665, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0022890893481726238\n",
      "epoch 1 step 901:\n",
      "action: tensor([0.0666, 0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0665, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.002379605745474578\n",
      "epoch 1 step 1001:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.007274658994964156\n",
      "epoch 1 step 1101:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668, 0.0667, 0.0667, 0.0666,\n",
      "        0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012950054530152855\n",
      "epoch 1 step 1201:\n",
      "action: tensor([0.0668, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006022289707200201\n",
      "epoch 1 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0026622328625300895\n",
      "epoch 1 step 1401:\n",
      "action: tensor([0.0666, 0.0666, 0.0666, 0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.001164432667023263\n",
      "epoch 1 step 1501:\n",
      "action: tensor([0.0666, 0.0666, 0.0666, 0.0667, 0.0668, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0011983886051538661\n",
      "epoch 1 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0666, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006386368950774823\n",
      "epoch 1 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0668, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.03492835589140775\n",
      "epoch 1 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.009317323406250466\n",
      "epoch 1 step 1901:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0665, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0668, 0.0667, 0.0667, 0.0667, 0.0668, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002792297807488353\n",
      "epoch 1 step 2001:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0665, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0666, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.00543854935591348\n",
      "=================================\n",
      "the profit margin is 205.73052348828563 %\n",
      "the sharpe ratio is 2.964563952491322\n",
      "the Volatility is 0.00902999897980343\n",
      "the max drawdown is 0.6740645091929746\n",
      "the Calmar Ratio is 1.7800697986576448\n",
      "the Sortino Ratio is 3.8876196293706067\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.73660656664603 %\n",
      "the sharpe ratio is 2.9646050233529304\n",
      "the Volatility is 0.00903002769780274\n",
      "the max drawdown is 0.6740709941577725\n",
      "the Calmar Ratio is 1.780082995209197\n",
      "the Sortino Ratio is 3.8876659038580397\n",
      "=================================\n",
      "epoch 2 step 1:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0034295454947557147\n",
      "epoch 2 step 101:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668,\n",
      "        0.0666, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.029036937732881185\n",
      "epoch 2 step 201:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014295453437409122\n",
      "epoch 2 step 301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008274357338610372\n",
      "epoch 2 step 401:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006353126968379641\n",
      "epoch 2 step 501:\n",
      "action: tensor([0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0031189416959733762\n",
      "epoch 2 step 601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0013255789767114834\n",
      "epoch 2 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.02114036819309284\n",
      "epoch 2 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0665, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0022894507453017354\n",
      "epoch 2 step 901:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0665, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.00237959577593827\n",
      "epoch 2 step 1001:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.007274644501348959\n",
      "epoch 2 step 1101:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012950207386133172\n",
      "epoch 2 step 1201:\n",
      "action: tensor([0.0668, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006023481606497683\n",
      "epoch 2 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.002661955650969716\n",
      "epoch 2 step 1401:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0011643523318038973\n",
      "epoch 2 step 1501:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0011993332480777497\n",
      "epoch 2 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006397815704186627\n",
      "epoch 2 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.034928900315245315\n",
      "epoch 2 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.009317364803493433\n",
      "epoch 2 step 1901:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002790671905085418\n",
      "epoch 2 step 2001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0665, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.005439374731224689\n",
      "=================================\n",
      "the profit margin is 205.75988689594263 %\n",
      "the sharpe ratio is 2.9647553574405414\n",
      "the Volatility is 0.009030161266994224\n",
      "the max drawdown is 0.6740962505311566\n",
      "the Calmar Ratio is 1.7801328955709244\n",
      "the Sortino Ratio is 3.887875574610034\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.76434211732942 %\n",
      "the sharpe ratio is 2.9647695195778305\n",
      "the Volatility is 0.009030238380136843\n",
      "the max drawdown is 0.674100999199182\n",
      "the Calmar Ratio is 1.78014406028258\n",
      "the Sortino Ratio is 3.8879000253127547\n",
      "=================================\n",
      "epoch 3 step 1:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.00342931151365633\n",
      "epoch 3 step 101:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668,\n",
      "        0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.02903825724782294\n",
      "epoch 3 step 201:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014295450613992955\n",
      "epoch 3 step 301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008274425858049383\n",
      "epoch 3 step 401:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006353165891191281\n",
      "epoch 3 step 501:\n",
      "action: tensor([0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0031186249611021566\n",
      "epoch 3 step 601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.001325740859709157\n",
      "epoch 3 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.02114012603906268\n",
      "epoch 3 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0022897706477849056\n",
      "epoch 3 step 901:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0023788383831817583\n",
      "epoch 3 step 1001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.00727473454410088\n",
      "epoch 3 step 1101:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012950115514620109\n",
      "epoch 3 step 1201:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006022403591206427\n",
      "epoch 3 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0026624553042413623\n",
      "epoch 3 step 1401:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.001165281970113341\n",
      "epoch 3 step 1501:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0011993237510221633\n",
      "epoch 3 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006394914666749685\n",
      "epoch 3 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.03492756979345657\n",
      "epoch 3 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.009317690421893232\n",
      "epoch 3 step 1901:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002789577807924104\n",
      "epoch 3 step 2001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.005439714512727178\n",
      "=================================\n",
      "the profit margin is 205.76481287914837 %\n",
      "the sharpe ratio is 2.964771284080382\n",
      "the Volatility is 0.009030245525050377\n",
      "the max drawdown is 0.6741019579799455\n",
      "the Calmar Ratio is 1.780143996318308\n",
      "the Sortino Ratio is 3.8879114672142547\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.7556555527047 %\n",
      "the sharpe ratio is 2.964688046597435\n",
      "the Volatility is 0.009030277837905307\n",
      "the max drawdown is 0.6740921973926794\n",
      "the Calmar Ratio is 1.78012616270012\n",
      "the Sortino Ratio is 3.887600276923176\n",
      "=================================\n",
      "epoch 4 step 1:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0034297256674964416\n",
      "epoch 4 step 101:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.029037043331587853\n",
      "epoch 4 step 201:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014295780178629869\n",
      "epoch 4 step 301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008274641345488831\n",
      "epoch 4 step 401:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006351640813782922\n",
      "epoch 4 step 501:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0031182285774775664\n",
      "epoch 4 step 601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.001325244527441427\n",
      "epoch 4 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.021139379197963493\n",
      "epoch 4 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0022900090975888077\n",
      "epoch 4 step 901:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.002378632023184224\n",
      "epoch 4 step 1001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.007274722845167503\n",
      "epoch 4 step 1101:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012950323599216773\n",
      "epoch 4 step 1201:\n",
      "action: tensor([0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006023378313013694\n",
      "epoch 4 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0026619758079657885\n",
      "epoch 4 step 1401:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.001164533039766269\n",
      "epoch 4 step 1501:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0011995525849908262\n",
      "epoch 4 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006397931729491546\n",
      "epoch 4 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.03492831894606496\n",
      "epoch 4 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.009317476567259675\n",
      "epoch 4 step 1901:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002790379285926292\n",
      "epoch 4 step 2001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0054397135022625775\n",
      "=================================\n",
      "the profit margin is 205.75766153990313 %\n",
      "the sharpe ratio is 2.964706031367568\n",
      "the Volatility is 0.009030271682732174\n",
      "the max drawdown is 0.6740941099268415\n",
      "the Calmar Ratio is 1.780130697574973\n",
      "the Sortino Ratio is 3.8876039864669303\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.7751456470909 %\n",
      "the sharpe ratio is 2.964830489048972\n",
      "the Volatility is 0.009030331351372734\n",
      "the max drawdown is 0.6741127451023824\n",
      "the Calmar Ratio is 1.7801679776514823\n",
      "the Sortino Ratio is 3.8879909703669413\n",
      "=================================\n",
      "epoch 5 step 1:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.003429350873885184\n",
      "epoch 5 step 101:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.029039307819884286\n",
      "epoch 5 step 201:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014295033905142418\n",
      "epoch 5 step 301:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008274045578884781\n",
      "epoch 5 step 401:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006353978534647808\n",
      "epoch 5 step 501:\n",
      "action: tensor([0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0668, 0.0667, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0031189317812998496\n",
      "epoch 5 step 601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0667, 0.0665, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0013273244704627984\n",
      "epoch 5 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.021142353964201277\n",
      "epoch 5 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0665, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0022883700636100457\n",
      "epoch 5 step 901:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0665, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.002380557819497753\n",
      "epoch 5 step 1001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.007274549722430024\n",
      "epoch 5 step 1101:\n",
      "action: tensor([0.0668, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012950613194938398\n",
      "epoch 5 step 1201:\n",
      "action: tensor([0.0668, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006024494854893092\n",
      "epoch 5 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0026617879925918686\n",
      "epoch 5 step 1401:\n",
      "action: tensor([0.0666, 0.0666, 0.0667, 0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0665, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0011639607839271093\n",
      "epoch 5 step 1501:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.001199753042168794\n",
      "epoch 5 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0668,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006403806909762721\n",
      "epoch 5 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0666, 0.0667, 0.0668,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.03492919912563508\n",
      "epoch 5 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.009317053819584586\n",
      "epoch 5 step 1901:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0665, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0027925958132470896\n",
      "epoch 5 step 2001:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0665, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.005439471415169095\n",
      "=================================\n",
      "the profit margin is 205.7852948702964 %\n",
      "the sharpe ratio is 2.9649069897099083\n",
      "the Volatility is 0.009030351004018453\n",
      "the max drawdown is 0.6741232275260464\n",
      "the Calmar Ratio is 1.7801901031980256\n",
      "the Sortino Ratio is 3.888076130252173\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.77957135892063 %\n",
      "the sharpe ratio is 2.9648597224876365\n",
      "the Volatility is 0.009030354663649784\n",
      "the max drawdown is 0.674117127839903\n",
      "the Calmar Ratio is 1.7801785520630575\n",
      "the Sortino Ratio is 3.8880077090056036\n",
      "=================================\n",
      "epoch 6 step 1:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.003428910810535868\n",
      "epoch 6 step 101:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668,\n",
      "        0.0666, 0.0667, 0.0666, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.029040219706043757\n",
      "epoch 6 step 201:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0668, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0668, 0.0667, 0.0667, 0.0664, 0.0666, 0.0668],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.014294842513637107\n",
      "epoch 6 step 301:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0666, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.008273725146006328\n",
      "epoch 6 step 401:\n",
      "action: tensor([0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006354227549902447\n",
      "epoch 6 step 501:\n",
      "action: tensor([0.0666, 0.0666, 0.0667, 0.0667, 0.0666, 0.0668, 0.0667, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0031190626605877014\n",
      "epoch 6 step 601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0013271538813466321\n",
      "epoch 6 step 701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0668, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0665, 0.0666, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.021142340457497966\n",
      "epoch 6 step 801:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0668, 0.0666, 0.0666, 0.0668,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0665, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002288690216518674\n",
      "epoch 6 step 901:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0665, 0.0666,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0023800777801010753\n",
      "epoch 6 step 1001:\n",
      "action: tensor([0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.007274577040867314\n",
      "epoch 6 step 1101:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0666, 0.0668, 0.0667, 0.0666, 0.0667,\n",
      "        0.0666, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.012950533347851234\n",
      "epoch 6 step 1201:\n",
      "action: tensor([0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.006023372712014918\n",
      "epoch 6 step 1301:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0026621838901341732\n",
      "epoch 6 step 1401:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0668, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.001164563541507846\n",
      "epoch 6 step 1501:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.0011993753807306717\n",
      "epoch 6 step 1601:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0666],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.0006397779371578594\n",
      "epoch 6 step 1701:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.034928469129019035\n",
      "epoch 6 step 1801:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667, 0.0667, 0.0667,\n",
      "        0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.00931737455428383\n",
      "epoch 6 step 1901:\n",
      "action: tensor([0.0667, 0.0666, 0.0667, 0.0666, 0.0667, 0.0667, 0.0666, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0667, 0.0667, 0.0667, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: -0.002790781952340282\n",
      "epoch 6 step 2001:\n",
      "action: tensor([0.0667, 0.0667, 0.0667, 0.0666, 0.0667, 0.0667, 0.0667, 0.0666, 0.0667,\n",
      "        0.0667, 0.0667, 0.0666, 0.0666, 0.0666, 0.0667],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "reward: 0.005439823658019094\n",
      "=================================\n",
      "the profit margin is 205.77292754388753 %\n",
      "the sharpe ratio is 2.9648198912167274\n",
      "the Volatility is 0.009030305575105\n",
      "the max drawdown is 0.6741104183974007\n",
      "the Calmar Ratio is 1.7801626773468595\n",
      "the Sortino Ratio is 3.887972319108152\n",
      "=================================\n",
      "=================================\n",
      "the profit margin is 205.77086295220047 %\n",
      "the sharpe ratio is 2.9647958196615862\n",
      "the Volatility is 0.009030331502371819\n",
      "the max drawdown is 0.6741082179623608\n",
      "the Calmar Ratio is 1.7801591459559243\n",
      "the Sortino Ratio is 3.887948458819635\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "agent=trader()\n",
    "agent.train_with_valid()\n",
    "test_information_1=pd.read_csv(\"data/test_input_1.csv\",index_col=0)\n",
    "test_information_2=pd.read_csv(\"data/test_input_2.csv\",index_col=0)\n",
    "technical_indicator=[\"feature_0\",\"feature_1\",\"feature_2\",\"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\",\"feature_7\",\"feature_8\",\"feature_9\",\"feature_10\"]\n",
    "action_list_1=[]\n",
    "for date in test_information_1.index.unique():\n",
    "    s=test_information_1[test_information_1.index==date][technical_indicator].values\n",
    "    shape=s.shape\n",
    "    s=s.reshape(shape[0],1,shape[1])\n",
    "    s=torch.from_numpy(s).float()\n",
    "    action=agent.net(s)\n",
    "    #here the origional action for the environment consider the cash, which is more pracitical in real world, but for \n",
    "    #the competition, we only need the last 15 weights for the assets, therefore we need to normalize the result as well\n",
    "    action=action.detach().float().numpy()\n",
    "    action=action[:]/np.sum(action[:])\n",
    "    action_list_1.append(action)\n",
    "action_list_1=np.array(action_list_1)\n",
    "action_list_1=action_list_1.astype(float)\n",
    "# action_list_1 = check_action_score(action_list_1)\n",
    "np.save(result_folder_location + \"action1.npy\",action_list_1)\n",
    "action_list_2=[]\n",
    "for date in test_information_2.index.unique():\n",
    "    s=test_information_2[test_information_2.index==date][technical_indicator].values\n",
    "    shape=s.shape\n",
    "    s=s.reshape(shape[0],1,shape[1])\n",
    "    s=torch.from_numpy(s).float()\n",
    "    action=agent.net(s)\n",
    "    #here the origional action for the environment consider the cash, which is more pracitical in real world, but for \n",
    "    #the competition, we only need the last 15 weights for the assets, therefore we need to normalize the result as well\n",
    "    action=action.detach().float().numpy()\n",
    "    action=action[:]/np.sum(action[:])\n",
    "    action_list_2.append(action)\n",
    "action_list_2=np.array(action_list_2)\n",
    "action_list_2=action_list_2.astype(float)\n",
    "# action_list_2 = check_action_score(action_list_2)\n",
    "np.save(result_folder_location + \"action2.npy\",action_list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Compress the 2 file as a zip file as submission file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05445587, 0.05445835, 0.0544646 , 0.0544553 , 0.05446438,\n",
       "        0.05445864, 0.05445456, 0.05446233, 0.05444942, 0.05445903,\n",
       "        0.05446204, 0.05445822, 0.05444945, 0.05445861, 0.23758923],\n",
       "       [0.05446298, 0.05445327, 0.05445558, 0.05445295, 0.0544682 ,\n",
       "        0.05446093, 0.05445122, 0.05445627, 0.05445294, 0.0544655 ,\n",
       "        0.05445802, 0.05445875, 0.05445709, 0.05446165, 0.2375847 ],\n",
       "       [0.05445847, 0.05445776, 0.05446088, 0.05445886, 0.05445499,\n",
       "        0.05445784, 0.0544608 , 0.05446146, 0.05445971, 0.05445395,\n",
       "        0.05445489, 0.0544585 , 0.05445861, 0.0544601 , 0.23758319],\n",
       "       [0.05446056, 0.05445739, 0.0544563 , 0.05445885, 0.05445674,\n",
       "        0.05445748, 0.05445997, 0.05446076, 0.05445714, 0.05446104,\n",
       "        0.05445932, 0.05445834, 0.05445796, 0.05445762, 0.23758048],\n",
       "       [0.05445695, 0.05445645, 0.05446095, 0.05446024, 0.05445758,\n",
       "        0.05445768, 0.05446044, 0.05445965, 0.05446013, 0.05445645,\n",
       "        0.054458  , 0.05446214, 0.05445969, 0.05445604, 0.23757763],\n",
       "       [0.05446046, 0.0544626 , 0.05446411, 0.05446022, 0.0544555 ,\n",
       "        0.0544549 , 0.05446112, 0.05445936, 0.0544566 , 0.05445873,\n",
       "        0.05445855, 0.05445839, 0.05445978, 0.05445499, 0.2375747 ],\n",
       "       [0.05445989, 0.05445553, 0.05445583, 0.05446081, 0.05445681,\n",
       "        0.05446137, 0.05445343, 0.05446122, 0.05446237, 0.05445953,\n",
       "        0.05446091, 0.05445475, 0.05445883, 0.05446137, 0.23757732],\n",
       "       [0.05445883, 0.05445636, 0.05445625, 0.05445687, 0.05445741,\n",
       "        0.0544578 , 0.0544568 , 0.05446394, 0.05446044, 0.05445761,\n",
       "        0.05445815, 0.05446183, 0.05446012, 0.05445864, 0.23757893],\n",
       "       [0.05445584, 0.05445881, 0.05445763, 0.05445791, 0.05446243,\n",
       "        0.05445385, 0.05445844, 0.05445752, 0.05446066, 0.05445961,\n",
       "        0.05446039, 0.05445429, 0.05445828, 0.05445813, 0.23758622]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_list_2 = np.load(\"result/\"+ model_name + \"/test_result/action2.npy\")\n",
    "action_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ef60ae2acd7272b700e0d59c8441536db607a994a33d88fe9cbde0933029ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
